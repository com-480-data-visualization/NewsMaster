{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSS News Feeds Analysis Notebook\n",
    "\n",
    "This notebook pulls multiple RSS feeds, aggregates the data, and performs basic analysis and visualization.  \n",
    "To gather a dataset, we plan to use a github action that every will run and store the news a json.  \n",
    "This will allow us to compare and analyze trend in the medias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# List your RSS feed URLs here\n",
    "feed_urls = [\n",
    "    \"https://feeds.bbci.co.uk/news/rss.xml\",\n",
    "    \"https://reutersbest.com/feed/\",\n",
    "    \"https://www.euronews.com/rss\",\n",
    "    \"https://www.wired.com/feed/rss\",\n",
    "    \"https://news.google.com/rss?hl=en-US&gl=US&ceid=US:en\",\n",
    "    \"https://rss.nytimes.com/services/xml/rss/nyt/HomePage.xml\"\n",
    "\n",
    "]\n",
    "\n",
    "# List to hold all the feed entries\n",
    "entries = []\n",
    "\n",
    "# Pull and parse each feed\n",
    "for url in feed_urls:\n",
    "    try:\n",
    "        feed = feedparser.parse(url)\n",
    "        if 'title' not in feed.feed:\n",
    "            print(f\"Skipping feed {url} as it does not contain a title.\")\n",
    "            continue\n",
    "        source_title = feed.feed.get('title', 'Unknown Source')\n",
    "        for entry in feed.entries:\n",
    "            entries.append({\n",
    "                'source': source_title,\n",
    "                'title': entry.get('title', ''),\n",
    "                'published': entry.get('published', entry.get('updated', entry.get('pubDate', ''))),\n",
    "                'link': entry.get('link', ''),\n",
    "                'summary': entry.get('summary', entry.get('description', ''))\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing feed {url}: {e}\")\n",
    "\n",
    "# Create a DataFrame from the entries\n",
    "df = pd.DataFrame(entries)\n",
    "\n",
    "# Convert the published date to a datetime object, handling errors gracefully\n",
    "df['published'] = pd.to_datetime(df['published'], errors='coerce')\n",
    "\n",
    "# Display a sample of the data\n",
    "print('Sample entries:')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Analysis and Statistics\n",
    "\n",
    "We now perform some basic analysis on the aggregated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Count of news items per source\n",
    "news_count = df['source'].value_counts()\n",
    "print('News count per source:')\n",
    "display(news_count)\n",
    "\n",
    "# Extract the words count of each news, the title and the summary\n",
    "df['word_count'] = df['title'].str.split().str.len()\n",
    "df['word_count'] = df['summary'].str.split().str.len() + df['word_count']\n",
    "# visualize the word count\n",
    "df['word_count'].plot(kind='hist', bins=20)\n",
    "plt.title('Word count distribution')\n",
    "plt.xlabel('Word count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "- A bar chart showing the number of news items per source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Number of News Items per Source\n",
    "plt.figure(figsize=(10, 5))\n",
    "news_count.plot(kind='bar')\n",
    "plt.title('Number of News Items per Source')\n",
    "plt.xlabel('Source')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2211efe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de12bbb",
   "metadata": {},
   "source": [
    "Next, let's look at the similarity between each article descriptions using cosine similarity.  \n",
    "This may give us insight in how article each relate to each others.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9054eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Prepare English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Drop duplicates and null titles\n",
    "df_titles = df[['title']].dropna().drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Basic text cleaning function\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())  # Remove punctuation and lowercase\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n",
    "    return tokens\n",
    "\n",
    "df_titles['tokens'] = df_titles['title'].apply(preprocess)\n",
    "\n",
    "# Flatten list of tokens to get word frequencies\n",
    "all_tokens = [token for sublist in df_titles['tokens'] for token in sublist]\n",
    "token_freq = Counter(all_tokens)\n",
    "\n",
    "# Filter out infrequent words\n",
    "df_titles['filtered_tokens'] = df_titles['tokens'].apply(\n",
    "    lambda tokens: [t for t in tokens if token_freq[t] > 1]\n",
    ")\n",
    "\n",
    "# Convert token lists back to strings\n",
    "df_titles['cleaned_title'] = df_titles['filtered_tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# TF-IDF Vectorization on cleaned titles\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df_titles['cleaned_title'])\n",
    "\n",
    "# Compute similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix)\n",
    "\n",
    "# Plot the similarity heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cosine_sim, cmap='viridis')\n",
    "plt.title(\"Cosine Similarity Between Cleaned Article Titles\")\n",
    "plt.xlabel(\"Article Index\")\n",
    "plt.ylabel(\"Article Index\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1234034",
   "metadata": {},
   "source": [
    "As we can see, this is quite disapointing but expected.   \n",
    "As new from different outlets may use quite a different wording.  \n",
    "To counter this we can use emeding model.   \n",
    "There a neat libary call [BERTopic](https://maartengr.github.io/BERTopic/index.html) to do so.  \n",
    "Here is sneak peak of what it can do : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1024410",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6792b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Use the same cleaned titles from the previous step\n",
    "titles_cleaned = df_titles['cleaned_title'].tolist()\n",
    "\n",
    "# Create and fit BERTopic model\n",
    "topic_model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
    "topics, probs = topic_model.fit_transform(titles_cleaned)\n",
    "\n",
    "# Visualize topics in different ways\n",
    "topic_model.visualize_topics()  \n",
    "\n",
    "topic_model.visualize_barchart(top_n_topics=10)\n",
    "\n",
    "topic_model.visualize_heatmap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a2f193",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
